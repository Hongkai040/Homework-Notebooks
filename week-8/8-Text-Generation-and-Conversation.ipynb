{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Text Generation and Conversation\n",
    "Many natural language activities boil down to text generation, especially the back-and-forth nature of natural conversation and question answering. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n",
    "\n",
    "These models can be quite impressive, even uncanny in how human like they sound. Check out this [cool website](https://transformer.huggingface.co), which allows you to write with a transformer. The website is built by the folks who wrote the package we are using. The code underneath the website can be found in their examples: [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py).\n",
    "\n",
    "Much 2022 NLP research is on text generation. Most famously, this is the primary use of large language models like GPT-3 (OpenAI), Wu Dao (Beijing Academy of AI), and Gopher (DeepMind)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "\n",
    "import sklearn #For generating some matrices\n",
    "import pandas as pd #For DataFrames\n",
    "import numpy as np #For arrays\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "import seaborn #Makes the plots look nice\n",
    "import scipy #Some stats\n",
    "import nltk #a little language code\n",
    "from IPython.display import Image #for pics\n",
    "import pickle #if you want to save layouts\n",
    "import os\n",
    "import io\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch # pip install torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig # pip install tranformers==2.4.1\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactional influence\n",
    "\n",
    "Before we utilize transformers, let's see how to estimate the influence of one speaker on another in order to estimate a kind of interpersonal influence network based on a recent paper by Fangjian Guo, Charles Blundell, Hanna Wallach, and Katherine Heller entitled [\"The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation\"](https://arxiv.org/pdf/1411.2674.pdf). This relies on a kind of point process called a Hawkes process that estimate the influence of one point on another. Specifically, what they estimate is the degree to which one actor to an interpersonal interaction engaged in \"accomodation\" behaviors relative to the other, generating a directed edge from the one to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's look at the output of their analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_name = '12-angry-men'   #example datasets: \"12-angry-men\" or \"USpresident\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = '../data/Bayesian-echo/results/{}/'.format(example_name)\n",
    "if not os.path.isdir(result_path):\n",
    "    raise ValueError('Invalid example selected, only \"12-angry-men\" or \"USpresident\" are avaliable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_info = pd.read_table(result_path + 'meta-info.txt',header=None)\n",
    "df_log_prob = pd.read_csv(result_path + \"SAMPLE-log_prior_and_log_likelihood.txt\",delim_whitespace=True) #log_prob samples\n",
    "df_influence = pd.read_csv(result_path + 'SAMPLE-influence.txt',delim_whitespace=True) # influence samples\n",
    "df_participants = pd.read_csv(result_path + 'cast.txt', delim_whitespace=True)\n",
    "person_id = pd.Series(df_participants['agent.num'].values-1,index=df_participants['agent.name']).to_dict()\n",
    "print()\n",
    "print ('Person : ID')\n",
    "person_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDensity(df):\n",
    "    data = df#_log_prob['log.prior']\n",
    "    density = scipy.stats.gaussian_kde(data)\n",
    "    width = np.max(data) - np.min(data)\n",
    "    xs = np.linspace(np.min(data)-width/5, np.max(data)+width/5,600)\n",
    "    density.covariance_factor = lambda : .25\n",
    "    density._compute_covariance()\n",
    "    return xs, density(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot MCMC (Markov Monte Carlo) trace and the density of log-likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,10])\n",
    "\n",
    "plt.subplot(4,2,1)\n",
    "plt.plot(df_log_prob['log.prior'])\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Trace of log.prior')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "x,y = getDensity(df_log_prob['log.prior'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.prior')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "plt.plot(df_log_prob['log.likelihood'])\n",
    "plt.title('Trace of log.likelihood')\n",
    "plt.xlabel('Iterations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "x,y = getDensity(df_log_prob['log.likelihood'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.likelihood')\n",
    "\n",
    "plt.subplot(4,2,5)\n",
    "plt.plot(df_log_prob['log.likelihood.test.set'])\n",
    "plt.title('Trace of log.likelihood.test.set')\n",
    "plt.xlabel('Iterations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(4,2,6)\n",
    "x,y = getDensity(df_log_prob['log.likelihood.test.set'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.likelihood.test.set')\n",
    "\n",
    "plt.subplot(4,2,7)\n",
    "plt.plot(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n",
    "plt.title('Trace of log.prob')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "plt.subplot(4,2,8)\n",
    "x,y = getDensity(df_log_prob['log.prior']+df_log_prob['log.likelihood'])\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Density of log.prob')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the influence matrix between participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = int(np.sqrt(len(df_influence.columns))) #number of participants\n",
    "id_person = {}\n",
    "for p in person_id:\n",
    "    id_person[person_id[p]]=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmatrix(stacked,A):\n",
    "    influence_matrix = [[0 for i in range(A)] for j in range(A)]\n",
    "    for row in stacked.iteritems():\n",
    "        from_ = int(row[0].split('.')[1])-1\n",
    "        to_ = int(row[0].split('.')[2])-1\n",
    "        value = float(row[1])\n",
    "        influence_matrix[from_][to_]=value\n",
    "    df_ = pd.DataFrame(influence_matrix) \n",
    "    \n",
    "    df_ =df_.rename(index = id_person)\n",
    "    df_ =df_.rename(columns = id_person)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = df_influence.mean(axis=0)\n",
    "df_mean = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.std(axis=0)\n",
    "df_std = getmatrix(stacked,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "seaborn.heatmap(df_mean, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n",
    "print('MEAN of influence matrix (row=from, col=to)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "seaborn.heatmap(df_std, annot=True,  linewidths=.5, ax=ax,cmap=\"YlGnBu\")\n",
    "print('SD of influence matrix (row=from, col=to)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot of total influences sent/received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_std = {} #sd of total influence sent\n",
    "reciever_std = {} #sd of total influence recieved\n",
    "for i in range(A):\n",
    "    reciever_std[id_person[i]] = df_influence[df_influence.columns[i::A]].sum(axis=1).std()\n",
    "    sender_std[id_person[i]] = df_influence[df_influence.columns[i*A:(i+1)*A:]].sum(axis=1).std()\n",
    "\n",
    "sent = df_mean.sum(axis=1) #mean of total influence sent\n",
    "recieved =df_mean.sum(axis=0) #mean of total influence recieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total influence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\t\\tTotal linguistic influence sent/received \")\n",
    "ax.fig = plt.figure(figsize=[np.min([A,20]),6])\n",
    "\n",
    "plt.grid()\n",
    "wd=0.45\n",
    "ii=0\n",
    "for p in sender_std:\n",
    "    plt.bar(person_id[p],sent.loc[p],width=wd,color='red',alpha=0.6,label = \"Sent\" if ii == 0 else \"\")\n",
    "    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]+sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n",
    "    plt.plot([person_id[p]-wd/4,person_id[p]+wd/4],[sent.loc[p]-sender_std[p],sent.loc[p]-sender_std[p]],color='k')\n",
    "    plt.plot([person_id[p],person_id[p]],[sent.loc[p]-sender_std[p],sent.loc[p]+sender_std[p]],color='k')\n",
    "    ii+=1\n",
    "ii=0\n",
    "for p in reciever_std:\n",
    "    plt.bar(person_id[p]+wd,recieved.loc[p],width=wd,color='blue',alpha=0.4,label = \"Received\" if ii == 0 else \"\")\n",
    "    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]+reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n",
    "    plt.plot([person_id[p]+wd-wd/4,person_id[p]+wd+wd/4],[recieved.loc[p]-reciever_std[p],recieved.loc[p]-reciever_std[p]],color='k')\n",
    "    plt.plot([person_id[p]+wd,person_id[p]+wd],[recieved.loc[p]-reciever_std[p],recieved.loc[p]+reciever_std[p]],color='k')\n",
    "    ii+=1\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.7))\n",
    "plt.xticks([i+0.25 for i in range(A)],list(zip(*sorted(id_person.items())))[1])\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('speaker',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Influence Network!\n",
    "\n",
    "You can visualize any of the influence matrices above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using networkx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawNetwork(df,title):\n",
    "    fig = plt.figure(figsize=[8,8])\n",
    "    G = nx.DiGraph()\n",
    "    for from_ in df.index:\n",
    "        for to_ in df.columns:\n",
    "            G.add_edge(from_,to_,weight = df.loc[from_][to_])\n",
    "            \n",
    "    pos = nx.spring_layout(G,k=0.55,iterations=20)\n",
    "    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "    weights = np.array(weights)\n",
    "    #weights = weights*weights\n",
    "    weights = 6*weights/np.max(weights)\n",
    "    print(title)\n",
    "    \n",
    "    edge_colors=20*(weights/np.max(weights))\n",
    "    edge_colors = edge_colors.astype(int)\n",
    "#     nx.draw_networkx_nodes(G,pos,node_size=1200,alpha=0.7,node_color='#99cef7')\n",
    "#     nx.draw_networkx_edges(G,pos,edge_color=edge_colors)\n",
    "#     nx.draw_networkx_labels(G,pos,font_weight='bold')\n",
    "    nx.draw(G,pos,with_labels=True, font_weight='bold',width=weights,\\\n",
    "            edge_color=255-edge_colors,node_color='#99cef7',node_size=1200,\\\n",
    "            alpha=0.75,arrows=True,arrowsize=20)\n",
    "    return edge_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quantile influence matrices for 25%, 50%, 75% quantile\n",
    "stacked = df_influence.quantile(0.25)\n",
    "df_q25 = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.quantile(0.5)\n",
    "df_q50 = getmatrix(stacked,A)\n",
    "\n",
    "stacked = df_influence.quantile(0.75)\n",
    "df_q75 = getmatrix(stacked,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_mean = drawNetwork(df_mean,'Mean Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_q25 = drawNetwork(df_q25,'25 Quantile Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_q75 = drawNetwork(df_q75,'75 Quantile Influence Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucem_illud_2020\n",
    "import pandas\n",
    "def fakeEnglish(length):\n",
    "    listd=['a','b','c','d','e','f','g','s','h','i','j','k','l']\n",
    "    return ''.join(np.random.choice(listd,length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your own dataset should contains 4 columns (with the same column names) as the artificial one below:\n",
    "\n",
    "- name: name of the participant\n",
    "- tokens: a list of tokens in one utterance\n",
    "- start: starting time of utterance (unit doesn't matter, can be 'seconds','minutes','hours'...)\n",
    "- end: ending time of utterance (same unit as start)\n",
    "\n",
    "There is no need to sort data for the moment.\n",
    "\n",
    "Below, we generate a fake collection of data from \"Obama\", \"Trump\", \"Clinton\"...and other recent presidents. You can either create your own simulation OR (better), add real interactional data from a online chat forum, comment chain, or transcribed from a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script= []\n",
    "language = 'eng' #parameter, no need to tune if using English, accept:{'eng','chinese'}\n",
    "role = 'Adult' #parameter, no need to tune \n",
    "\n",
    "for i in range(290):\n",
    "    dt = []\n",
    "    dt.append(np.random.choice(['Obama','Trump','Clinton','Bush','Reagan','Carter','Ford','Nixon','Kennedy','Roosevelt']))\n",
    "    faketokens = [fakeEnglish(length = 4) for j in range(30)]\n",
    "    dt.append(faketokens) #fake utterance\n",
    "    dt.append(i*2+np.random.random()) # start time\n",
    "    dt.append(i*2+1+np.random.random()) # end time\n",
    "    script.append(dt)\n",
    "\n",
    "df_transcript = pandas.DataFrame(script,columns=['name','tokens','start','end']) #\"start\", \"end\" are timestamps of utterances, units don't matter\n",
    "df_transcript[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data into TalkbankXML format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fname = 'USpresident.xml'  #should be .xml\n",
    "language = 'eng' \n",
    "#language = 'chinese'\n",
    "lucem_illud_2020.make_TalkbankXML(df_transcript, output_fname, language = language )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Bayesian Echo Chamber to get estimation.\n",
    "\n",
    "- It may take a couple of hours. ( About 4-5 hours if Vocab_size=600 and sampling_time =2000)\n",
    "- Larger \"Vocab_size\" (see below) will cost more time\n",
    "- Larger \"sampling_time\" will also consume more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab_size = 90 # up to Vocab_size most frequent words will be considered, it should be smaller than the total vocab\n",
    "sampling_time = 1500  #The times of Gibbs sampling sweeps  (500 burn-in not included)\n",
    "lucem_illud_2020.bec_run(output_fname, Vocab_size, language, sampling_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 1*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that perform a similar social similarity or influence analysis on a dataset relevant to your final project. Create relationships between actors in a network based on your dataset (e.g., person to person or document to document), and perform analyses that interrogate the structure of their interactions, similarity, and/or influence on one another. (For example, if relevant to your final project, you could explore different soap operas, counting how many times a character may have used the word love in conversation with another character, and identify if characters in love speak like each other. Or do opposites attract?) What does that analysis and its output reveal about the relative influence of each actor on others? What does it reveal about the social game being played?\n",
    "\n",
    "<font color=\"red\">Stretch 1:\n",
    "Render the social network with weights (e.g., based on the number of scenes in which actors appear together), then calculate the most central actors in the show.Realtime output can be viewed in shell.\n",
    "\n",
    "<font color=\"red\">Stretch 2:\n",
    "Implement more complex measures of similarity based on the papers you have read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation using GPT-2 and BERT\n",
    "\n",
    "We can make use of the transformers we learned about last week to do text generation, where the model takes one or multiple places in a conversation. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n",
    "\n",
    "These models can be quite impressive, even uncanny in how human like they sound. Check out this [cool website](https://transformer.huggingface.co), which allows you to write with a transformer. The website is built by the folks who wrote the package we are using. The code underneath the website can be found in their examples: [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py).\n",
    "\n",
    "We will be using the built in generate function, but the example file has more detailed code which allows you to set the seed differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Nothing that we like to do more than analyse data all day long and\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. A little creepy, and as we can see, far from perfect: GPT doesn't alwats work out flawlessly, but it sometimes can, and we will try and see if fine-tuning helps. We are going to tune the model on a complete dataset of Trump tweets, as they have a set of distinctive, highly identifiable qualities.\n",
    "\n",
    "### Creating a domain-specific language model\n",
    "\n",
    "One of the most exciting things about BERT and GPT is being able to retune them the way we want to. We will be training models to perform two tasks - one is to create a BERT with an \"accent\", by traning a model with english news data from the UK, from the US, and from India. We will also train a language generation model with a bunch of Trump tweets. \n",
    "\n",
    "We can train models specifically over a certain domain to make its language generation similar to that domain. \n",
    "[run_language modelling.py](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py), followed by [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py). I've downloaded these files and added them to this directory so we can run them through the notebook. You are encouraged to look at these files to get a rough idea of what is going on.\n",
    "\n",
    "### Loading Data \n",
    "\n",
    "We want to now get our Trump tweets and our English news datasets ready. The data the scripts expect is just a text file with relevant data. We load the Trump tweets and then write them to disk as train and test files with only data. I leave the original dataframes in case you would like to use it for your own purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"../data/trump_tweets\"):\n",
    "    dfs.append(pd.read_json(\"../data/trump_tweets/\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(df['text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text.to_frame().to_csv(r'train_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text.to_frame().to_csv(r'test_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now used the Google Colab GPUs to train the Trump tweet models. We'll be doing the same for our blog posts too.\n",
    "\n",
    "### GloWBe dataset\n",
    "\n",
    "We'll now load up the GloWbe (Corpus of Global Web-Based English) dataset which have different texts from different countries. We'll try and draw out texts from only the US, UK and India. We'll then save these to disk. Note that this is a Davies Corpora dataset: the full download can be done with the Dropbox link I sent in an announcement a few weeks ago. The whole download is about 3.5 GB but we only need two files, which are anout 250 MB each. The other files might be useful for your research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"/Users/bhargavvader/Downloads/Academics_tech/corpora/GloWbE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the exact name of the files\n",
    "us = \"/text_us_blog_jfy.zip\"\n",
    "gb = \"/text_gb_blog_akq.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_texts = lucem_illud.loadDavies(address, corpus_style=\"us_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_texts = lucem_illud.loadDavies(address, corpus_style=\"gb_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dictionary with document ids mapping to text. Since we don't need any information but the text, we can just save these to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(list(us_texts.values())[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_texts(texts, file_name):\n",
    "    text = []\n",
    "    for doc in list(texts.values()):\n",
    "        text.append(' '.join(doc).replace(\"< h >\", \"\").replace(\"< p >\", \"\"))\n",
    "    train_text, test_text = train_test_split(text, test_size=0.2)\n",
    "    with open(file_name + \"_train\", 'w') as f:\n",
    "        for item in train_text:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(file_name + \"_test\", 'w') as f:\n",
    "        for item in test_text:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_texts(us_texts, \"us_blog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_texts(gb_texts, \"gb_blog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the training and testing files for both US and GB blogs in English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING - SHIFT TO GOOGLE COLAB OR GPU ENABLED MACHINE\n",
    "\n",
    "The \\[TO-DO: update this like once we have new notebooks set up\\][Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) walks you through the process of fine-tuning models, as we did before for the classification task. Move now to the colab file to fine tune your models. Once you downloaded all the models and their information, place those files in the directory of the HW to use them as demonstrated below. \n",
    "\n",
    "\n",
    "\n",
    "### Running Scripts\n",
    "\n",
    "We use the scripts to do language modelling and text generation. The following cells run the code as if you would have run it in a terminal. I trained all of these models using the Googlr Colab file, and then saved the models to disk.\n",
    "\n",
    "#### Trump GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=train_text_trump --do_eval --eval_data_file=test_text_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_language_modeling.py --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_language_modeling.py --output_dir=output_roberta_UK --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
    "\n",
    "### Loading and using models\n",
    "\n",
    "Let us now load the four models we have and see how we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now - let us see what our Trump Tweet Bot looks like!\n",
    "You can generate text via command line using the command below. You can also load a model once it is saved - I trained my model using Google Colab, downloaded the model, and am loading it again via the command below. Note that you have to download all the files in your folder of the fine-tuned model to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run_generation.py --model_type=gpt2 --model_name_or_path=output_trump_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_trump = AutoTokenizer.from_pretrained(\"output_trump_gpt\")\n",
    "model_trump = AutoModelWithLMHead.from_pretrained(\"output_trump_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_trump.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_trump.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_trump.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - our Trump bot is nasty, so we know our model trained well. What happens if we try the same sentence for our non-fine tuned model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite the contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Exercise 2*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that generate a BERT-powered chatbot tuned on text related to your final project. What is interesting about this model, and how to does it compare to an untrained model? What does it reveal about the social game involved with your dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
